{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ MiniLLM 0.2B æ¨¡å‹è®­ç»ƒ (A100 GPU)\n",
    "\n",
    "æœ¬ Colab Notebook åœ¨ A100 GPU ä¸Šä»é›¶è®­ç»ƒä¸€ä¸ª 0.2B (200M å‚æ•°) çš„ MiniLLM æ¨¡å‹ã€‚\n",
    "\n",
    "**åŠŸèƒ½ç‰¹ç‚¹:**\n",
    "- DeepSeek-V3.2 é£æ ¼æ¶æ„ (MLA + MoE + MTP)\n",
    "- æ”¯æŒé¢„è®­ç»ƒå’Œ SFT å¾®è°ƒ\n",
    "- é’ˆå¯¹ A100 80GB ä¼˜åŒ–çš„æ‰¹é‡å¤§å°\n",
    "- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ (bfloat16)\n",
    "\n",
    "**è¿è¡Œç¯å¢ƒè¦æ±‚:**\n",
    "- Colab Pro+ æˆ–è‡ªå»º A100 GPU ç¯å¢ƒ\n",
    "- Python 3.10+\n",
    "- PyTorch 2.0+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥ GPU ç±»å‹\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…‹éš† mini-llm ä»“åº“\n",
    "!git clone https://github.com/ai-clarify/mini-llm.git\n",
    "%cd mini-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "!pip install -q torch==2.6.0 transformers==4.57.1 datasets==2.21.0 \\\n",
    "    tokenizers==0.22.0 safetensors==0.4.5 einops==0.8.1 \\\n",
    "    tensorboard==2.17.1 tqdm==4.66.5 pyyaml==6.0.2 \\\n",
    "    huggingface_hub==0.34.0 wandb==0.18.3 swanlab==0.6.8\n",
    "\n",
    "# éªŒè¯å®‰è£…\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ æ¨¡å‹é…ç½® (0.2B å‚æ•°)\n",
    "\n",
    "è®¾è®¡ä¸€ä¸ªçº¦ 200M å‚æ•°çš„æ¨¡å‹:\n",
    "- `hidden_size=768`: éšè—å±‚ç»´åº¦\n",
    "- `num_hidden_layers=12`: Transformer å±‚æ•°\n",
    "- `num_attention_heads=12`: æ³¨æ„åŠ›å¤´æ•°\n",
    "- `q_lora_rank=256`: Q LoRA å‹ç¼©ç»´åº¦\n",
    "- `kv_lora_rank=128`: KV LoRA å‹ç¼©ç»´åº¦\n",
    "\n",
    "**å‚æ•°ä¼°ç®—:**\n",
    "- Embedding: vocab_size Ã— hidden_size = 6400 Ã— 768 â‰ˆ 5M\n",
    "- Attention: 12 å±‚ Ã— (MLA å‚æ•°) â‰ˆ 80M\n",
    "- FFN: 12 å±‚ Ã— (3 Ã— hidden_size Ã— intermediate) â‰ˆ 100M\n",
    "- æ€»è®¡: ~200M å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from model.model_minillm import MiniLLMConfig, MiniLLMForCausalLM\n",
    "\n",
    "# 0.2B æ¨¡å‹é…ç½®\n",
    "config_0_2b = MiniLLMConfig(\n",
    "    vocab_size=6400,\n",
    "    hidden_size=768,\n",
    "    intermediate_size=2048,  # ~2.67x hidden_size\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    \n",
    "    # MLA é…ç½®\n",
    "    q_lora_rank=256,\n",
    "    kv_lora_rank=128,\n",
    "    qk_nope_head_dim=64,\n",
    "    qk_rope_head_dim=32,\n",
    "    v_head_dim=64,\n",
    "    \n",
    "    # MTP (Multi-Token Prediction)\n",
    "    num_nextn_predict_layers=1,\n",
    "    mtp_loss_weight=0.1,\n",
    "    \n",
    "    # è®­ç»ƒç›¸å…³\n",
    "    max_position_embeddings=2048,\n",
    "    rms_norm_eps=1e-6,\n",
    "    dropout=0.0,\n",
    "    attention_dropout=0.0,\n",
    "    \n",
    "    # ä¸ä½¿ç”¨ MoE (ä¿æŒ Dense æ¨¡å‹)\n",
    "    use_moe=False,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹å¹¶è®¡ç®—å‚æ•°é‡\n",
    "model = MiniLLMForCausalLM(config_0_2b)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nğŸ“Š æ¨¡å‹ç»Ÿè®¡:\")\n",
    "print(f\"æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "print(f\"\\nğŸ“ æ¨¡å‹é…ç½®:\")\n",
    "print(f\"  hidden_size: {config_0_2b.hidden_size}\")\n",
    "print(f\"  num_hidden_layers: {config_0_2b.num_hidden_layers}\")\n",
    "print(f\"  num_attention_heads: {config_0_2b.num_attention_heads}\")\n",
    "print(f\"  intermediate_size: {config_0_2b.intermediate_size}\")\n",
    "print(f\"  max_position_embeddings: {config_0_2b.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ æ•°æ®å‡†å¤‡\n",
    "\n",
    "ä¸‹è½½å¹¶å‡†å¤‡é¢„è®­ç»ƒæ•°æ®é›†ã€‚å¯é€‰æ‹©:\n",
    "1. ä½¿ç”¨é¡¹ç›®è‡ªå¸¦çš„ç¤ºä¾‹æ•°æ®\n",
    "2. ä» HuggingFace ä¸‹è½½å…¬å¼€æ•°æ®é›†\n",
    "3. ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# åˆ›å»ºæ•°æ®ç›®å½•\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# é€‰é¡¹ 1: ä½¿ç”¨å°å‹ç¤ºä¾‹æ•°æ®å¿«é€ŸéªŒè¯\n",
    "USE_SAMPLE_DATA = True  # è®¾ä¸º False ä½¿ç”¨å®Œæ•´æ•°æ®\n",
    "\n",
    "if USE_SAMPLE_DATA:\n",
    "    # åˆ›å»ºä¸€ä¸ªå°å‹ç¤ºä¾‹æ•°æ®é›†ç”¨äºéªŒè¯æµç¨‹\n",
    "    sample_texts = [\n",
    "        \"äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œï¼Œæ·±åº¦å­¦ä¹ æ˜¯å…¶ä¸­æœ€é‡è¦çš„æŠ€æœ¯ä¹‹ä¸€ã€‚\",\n",
    "        \"å¤§è¯­è¨€æ¨¡å‹å¯ä»¥ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œå®ƒä»¬åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚\",\n",
    "        \"Transformer æ¶æ„æ˜¯ç°ä»£è¯­è¨€æ¨¡å‹çš„åŸºç¡€ï¼Œå®ƒä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥å¤„ç†åºåˆ—ã€‚\",\n",
    "        \"é¢„è®­ç»ƒå’Œå¾®è°ƒæ˜¯è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚\",\n",
    "        \"å¤šå¤´æ³¨æ„åŠ›å…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨ä¸åŒä½ç½®çš„ä¿¡æ¯ã€‚\",\n",
    "    ] * 1000  # å¤åˆ¶ä»¥åˆ›å»ºè¶³å¤Ÿçš„è®­ç»ƒæ•°æ®\n",
    "    \n",
    "    with open('data/pretrain_sample.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for text in sample_texts:\n",
    "            f.write(json.dumps({'text': text}, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    DATA_PATH = 'data/pretrain_sample.jsonl'\n",
    "    print(f\"âœ… å·²åˆ›å»ºç¤ºä¾‹æ•°æ®: {DATA_PATH}\")\n",
    "    print(f\"   æ ·æœ¬æ•°é‡: {len(sample_texts)}\")\n",
    "\n",
    "else:\n",
    "    # é€‰é¡¹ 2: ä» HuggingFace ä¸‹è½½ä¸­æ–‡é¢„è®­ç»ƒæ•°æ®\n",
    "    print(\"æ­£åœ¨ä¸‹è½½é¢„è®­ç»ƒæ•°æ®é›†...\")\n",
    "    \n",
    "    # ä½¿ç”¨ Wikipedia ä¸­æ–‡æˆ–å…¶ä»–å¼€æºæ•°æ®\n",
    "    # è¿™é‡Œä»¥ wikitext ä¸ºä¾‹ï¼Œå®é™…å¯æ›¿æ¢ä¸ºä¸­æ–‡æ•°æ®é›†\n",
    "    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "    \n",
    "    with open('data/pretrain_full.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            if item['text'].strip():\n",
    "                f.write(json.dumps({'text': item['text']}, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    DATA_PATH = 'data/pretrain_full.jsonl'\n",
    "    print(f\"âœ… æ•°æ®ä¸‹è½½å®Œæˆ: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æ•°æ®æ ¼å¼\n",
    "!head -3 {DATA_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ é¢„è®­ç»ƒ\n",
    "\n",
    "ä½¿ç”¨ `train_pretrain.py` è„šæœ¬è¿›è¡Œé¢„è®­ç»ƒã€‚\n",
    "\n",
    "**A100 80GB ä¼˜åŒ–å‚æ•°:**\n",
    "- `batch_size=64`: å¤§æ‰¹é‡æå‡ååé‡\n",
    "- `accumulation_steps=4`: ç­‰æ•ˆ batch_size=256\n",
    "- `max_seq_len=2048`: å®Œæ•´ä¸Šä¸‹æ–‡é•¿åº¦\n",
    "- `dtype=bfloat16`: A100 åŸç”Ÿæ”¯æŒçš„ç²¾åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒè¶…å‚æ•°\n",
    "TRAINING_CONFIG = {\n",
    "    # æ¨¡å‹å‚æ•° (0.2B)\n",
    "    'hidden_size': 768,\n",
    "    'num_hidden_layers': 12,\n",
    "    \n",
    "    # è®­ç»ƒå‚æ•°\n",
    "    'epochs': 3,\n",
    "    'batch_size': 64,           # A100 å¯æ”¯æŒå¤§æ‰¹é‡\n",
    "    'accumulation_steps': 4,    # ç­‰æ•ˆ batch_size=256\n",
    "    'learning_rate': 3e-4,\n",
    "    'max_seq_len': 2048,\n",
    "    \n",
    "    # ç²¾åº¦ä¸ä¼˜åŒ–\n",
    "    'dtype': 'bfloat16',        # A100 åŸç”Ÿæ”¯æŒ\n",
    "    'grad_clip': 1.0,\n",
    "    \n",
    "    # æ—¥å¿—ä¸ä¿å­˜\n",
    "    'log_interval': 50,\n",
    "    'save_interval': 500,\n",
    "    \n",
    "    # æ•°æ®è·¯å¾„\n",
    "    'data_path': DATA_PATH,\n",
    "    'out_dir': 'out/pretrain_0_2b',\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ è®­ç»ƒé…ç½®:\")\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "import os\n",
    "os.makedirs(TRAINING_CONFIG['out_dir'], exist_ok=True)\n",
    "\n",
    "# æ„å»ºè®­ç»ƒå‘½ä»¤\n",
    "train_cmd = f\"\"\"\n",
    "python trainer/train_pretrain.py \\\n",
    "    --hidden_size {TRAINING_CONFIG['hidden_size']} \\\n",
    "    --num_hidden_layers {TRAINING_CONFIG['num_hidden_layers']} \\\n",
    "    --epochs {TRAINING_CONFIG['epochs']} \\\n",
    "    --batch_size {TRAINING_CONFIG['batch_size']} \\\n",
    "    --accumulation_steps {TRAINING_CONFIG['accumulation_steps']} \\\n",
    "    --learning_rate {TRAINING_CONFIG['learning_rate']} \\\n",
    "    --max_seq_len {TRAINING_CONFIG['max_seq_len']} \\\n",
    "    --dtype {TRAINING_CONFIG['dtype']} \\\n",
    "    --grad_clip {TRAINING_CONFIG['grad_clip']} \\\n",
    "    --log_interval {TRAINING_CONFIG['log_interval']} \\\n",
    "    --save_interval {TRAINING_CONFIG['save_interval']} \\\n",
    "    --data_path {TRAINING_CONFIG['data_path']} \\\n",
    "    --out_dir {TRAINING_CONFIG['out_dir']} \\\n",
    "    --tensorboard_dir {TRAINING_CONFIG['out_dir']}/tensorboard \\\n",
    "    --device cuda:0\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸš€ è®­ç»ƒå‘½ä»¤:\")\n",
    "print(train_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹é¢„è®­ç»ƒ\n",
    "# æ³¨æ„: å®Œæ•´è®­ç»ƒå¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶åˆ°å‡ å¤©ï¼Œå–å†³äºæ•°æ®é‡\n",
    "!{train_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ ç›‘æ§è®­ç»ƒè¿›åº¦\n",
    "\n",
    "ä½¿ç”¨ TensorBoard å¯è§†åŒ–è®­ç»ƒæ›²çº¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½ TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir out/pretrain_0_2b/tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ SFT å¾®è°ƒ (å¯é€‰)\n",
    "\n",
    "é¢„è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥è¿›è¡Œç›‘ç£å¾®è°ƒ (SFT) æ¥æå‡å¯¹è¯èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡ SFT æ•°æ®é›†\n",
    "sft_samples = [\n",
    "    {\n",
    "        \"instruction\": \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\",\n",
    "        \"output\": \"äººå·¥æ™ºèƒ½(AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ ã€‚\",\n",
    "        \"output\": \"æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚è¡¨ç¤ºã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Transformer æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "        \"output\": \"Transformer æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥å¤„ç†åºåˆ—æ•°æ®ï¼Œæ˜¯ç°ä»£è¯­è¨€æ¨¡å‹çš„åŸºç¡€ã€‚\"\n",
    "    },\n",
    "] * 500\n",
    "\n",
    "with open('data/sft_sample.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in sft_samples:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"âœ… å·²åˆ›å»º SFT æ•°æ®: data/sft_sample.jsonl\")\n",
    "print(f\"   æ ·æœ¬æ•°é‡: {len(sft_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT è®­ç»ƒå‘½ä»¤\n",
    "sft_cmd = f\"\"\"\n",
    "python trainer/train_full_sft.py \\\n",
    "    --hidden_size 768 \\\n",
    "    --num_hidden_layers 12 \\\n",
    "    --epochs 3 \\\n",
    "    --batch_size 32 \\\n",
    "    --accumulation_steps 4 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_seq_len 2048 \\\n",
    "    --dtype bfloat16 \\\n",
    "    --data_path data/sft_sample.jsonl \\\n",
    "    --pretrained_path {TRAINING_CONFIG['out_dir']}/pretrain_768.pth \\\n",
    "    --out_dir out/sft_0_2b \\\n",
    "    --device cuda:0\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ¯ SFT è®­ç»ƒå‘½ä»¤:\")\n",
    "print(sft_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œ SFT (å–æ¶ˆæ³¨é‡Šä»¥æ‰§è¡Œ)\n",
    "# !{sft_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ æ¨¡å‹æ¨ç†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from model.model_minillm import MiniLLMConfig, MiniLLMForCausalLM\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„\n",
    "MODEL_PATH = f\"{TRAINING_CONFIG['out_dir']}/pretrain_768.pth\"\n",
    "\n",
    "# é‡æ–°åˆ›å»ºæ¨¡å‹é…ç½®\n",
    "config = MiniLLMConfig(\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    q_lora_rank=256,\n",
    "    kv_lora_rank=128,\n",
    ")\n",
    "\n",
    "model = MiniLLMForCausalLM(config)\n",
    "\n",
    "# åŠ è½½æƒé‡\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}\")\n",
    "    print(\"  è¯·å…ˆè¿è¡Œé¢„è®­ç»ƒæ­¥éª¤\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=100, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"ç”Ÿæˆæ–‡æœ¬\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "# æµ‹è¯•ç”Ÿæˆ\n",
    "test_prompts = [\n",
    "    \"äººå·¥æ™ºèƒ½\",\n",
    "    \"æ·±åº¦å­¦ä¹ æ˜¯\",\n",
    "    \"Transformer æ¨¡å‹\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ ç”Ÿæˆæµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "for prompt in test_prompts:\n",
    "    try:\n",
    "        result = generate_text(prompt, max_new_tokens=50)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Output: {result}\")\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"ç”Ÿæˆå¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ ä¿å­˜ä¸å¯¼å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹åˆ° HuggingFace æ ¼å¼ (å¯é€‰)\n",
    "EXPORT_DIR = 'out/minillm_0_2b_hf'\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "# ä¿å­˜é…ç½®\n",
    "config.save_pretrained(EXPORT_DIR)\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹æƒé‡\n",
    "torch.save(model.state_dict(), f'{EXPORT_DIR}/pytorch_model.bin')\n",
    "\n",
    "# å¤åˆ¶åˆ†è¯å™¨\n",
    "!cp model/tokenizer.json {EXPORT_DIR}/\n",
    "!cp model/tokenizer_config.json {EXPORT_DIR}/\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹å·²å¯¼å‡ºåˆ°: {EXPORT_DIR}\")\n",
    "!ls -la {EXPORT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‹ç¼©å¹¶ä¸‹è½½ (Colab)\n",
    "!zip -r minillm_0_2b.zip {EXPORT_DIR}\n",
    "\n",
    "# åœ¨ Colab ä¸­ä¸‹è½½\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('minillm_0_2b.zip')\n",
    "except ImportError:\n",
    "    print(\"ä¸åœ¨ Colab ç¯å¢ƒä¸­ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½: minillm_0_2b.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ è®­ç»ƒæŠ€å·§ä¸è°ƒä¼˜\n",
    "\n",
    "### A100 ä¼˜åŒ–å»ºè®®:\n",
    "\n",
    "1. **æ‰¹é‡å¤§å°**: A100 80GB å¯æ”¯æŒ `batch_size=128` + `max_seq_len=2048`\n",
    "2. **æ··åˆç²¾åº¦**: ä½¿ç”¨ `bfloat16` è·å¾—æœ€ä½³æ€§èƒ½\n",
    "3. **Flash Attention**: å¦‚æœå¯ç”¨ï¼Œå¯ç”¨ Flash Attention 2\n",
    "4. **æ¢¯åº¦ç´¯ç§¯**: ä½¿ç”¨ `accumulation_steps` æ¨¡æ‹Ÿæ›´å¤§æ‰¹é‡\n",
    "\n",
    "### åˆ†å¸ƒå¼è®­ç»ƒ:\n",
    "\n",
    "```bash\n",
    "# åŒå¡ DDP è®­ç»ƒ\n",
    "torchrun --nproc_per_node 2 trainer/train_pretrain.py \\\n",
    "    --ddp \\\n",
    "    --hidden_size 768 \\\n",
    "    --num_hidden_layers 12 \\\n",
    "    ...\n",
    "```\n",
    "\n",
    "### è°ƒè¯•æŠ€å·§:\n",
    "\n",
    "```python\n",
    "# ä½¿ç”¨ --max_steps è¿›è¡Œå¿«é€ŸéªŒè¯\n",
    "python trainer/train_pretrain.py --max_steps 100 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ å®Œæˆ\n",
    "\n",
    "æ­å–œï¼ä½ å·²ç»æˆåŠŸåœ¨ A100 ä¸Šè®­ç»ƒäº†ä¸€ä¸ª 0.2B å‚æ•°çš„ MiniLLM æ¨¡å‹ã€‚\n",
    "\n",
    "**åç»­æ­¥éª¤å»ºè®®:**\n",
    "1. ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†è¿›è¡Œå®Œæ•´é¢„è®­ç»ƒ\n",
    "2. è¿›è¡Œ SFT å¾®è°ƒä»¥æå‡å¯¹è¯èƒ½åŠ›\n",
    "3. å°è¯• DPO/GRPO ç­‰å¯¹é½æ–¹æ³•\n",
    "4. ä½¿ç”¨ vLLM/SGLang éƒ¨ç½²æ¨ç†æœåŠ¡\n",
    "\n",
    "**ç›¸å…³èµ„æº:**\n",
    "- [MiniLLM GitHub](https://github.com/ai-clarify/mini-llm)\n",
    "- [DeepSeek-V3 æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2401.xxxxx)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
