{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniLLM 0.2B Model Training (A100 GPU)\n",
    "\n",
    "This notebook trains a ~200M parameter MiniLLM model on Google Colab with A100 GPU.\n",
    "\n",
    "**Requirements:**\n",
    "- Colab Pro/Pro+ with A100 GPU\n",
    "- ~30GB disk space for data\n",
    "\n",
    "**Before running:** If you've installed other packages that might conflict, do **Runtime -> Factory reset runtime** first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://8080-gpu-a100-hm-2iklquhepr1z-a.europe-west4-0.prod.colab.dev/'. Verify the server is running and reachable. (HTTP 404: Not Found (Kernel does not exist: 091eee14-4087-42d7-9bfd-e7ca918025c1))."
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone repo\n",
    "if not os.path.exists('/content/mini-llm'):\n",
    "    !git clone https://github.com/ai-clarify/mini-llm.git /content/mini-llm\n",
    "else:\n",
    "    print('Repository already cloned')\n",
    "    !cd /content/mini-llm && git pull\n",
    "\n",
    "%cd /content/mini-llm\n",
    "\n",
    "# ============================================================\n",
    "# Install dependencies (Colab-optimized)\n",
    "# ============================================================\n",
    "# IMPORTANT: Don't reinstall torch - use Colab's pre-installed version\n",
    "# which has proper CUDA library linkage\n",
    "\n",
    "print(\"Installing additional dependencies...\")\n",
    "\n",
    "# Fix potential CUDA library issues\n",
    "!pip install -q nvidia-cusparselt-cu12 2>/dev/null || true\n",
    "\n",
    "# Core training dependencies\n",
    "!pip install -q einops safetensors modelscope datasets\n",
    "\n",
    "# Verify environment\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Environment check:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import torch\n",
    "print(f\"✓ PyTorch: {torch.__version__}\")\n",
    "print(f\"✓ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"✓ Transformers: {transformers.__version__}\")\n",
    "\n",
    "# Test model import\n",
    "import sys\n",
    "sys.path.insert(0, '/content/mini-llm')\n",
    "from model.model_minillm import MiniLLMConfig, MiniLLMForCausalLM\n",
    "print(\"✓ MiniLLM model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configuration (0.2B Parameters)\n",
    "\n",
    "We configure the model to have approximately 200M parameters:\n",
    "- `hidden_size=1024`: Embedding dimension\n",
    "- `num_hidden_layers=16`: Number of transformer blocks\n",
    "- `num_attention_heads=16`: Number of attention heads\n",
    "- `intermediate_size=2816`: FFN hidden dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/mini-llm')\n",
    "\n",
    "from model.model_minillm import MiniLLMConfig, MiniLLMForCausalLM\n",
    "\n",
    "# 0.2B Configuration\n",
    "MODEL_CONFIG = {\n",
    "    'hidden_size': 1024,\n",
    "    'num_hidden_layers': 16,\n",
    "    'num_attention_heads': 16,\n",
    "    'intermediate_size': 2816,\n",
    "    'q_lora_rank': 384,\n",
    "    'kv_lora_rank': 192,\n",
    "    'max_position_embeddings': 2048,\n",
    "    'use_moe': False,\n",
    "    'vocab_size': 6400,\n",
    "}\n",
    "\n",
    "# Verify parameter count\n",
    "config = MiniLLMConfig(**MODEL_CONFIG)\n",
    "model = MiniLLMForCausalLM(config)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params / 1e6:.1f}M\")\n",
    "print(f\"Trainable parameters: {trainable_params / 1e6:.1f}M\")\n",
    "print(f\"\\nModel config:\")\n",
    "for k, v in MODEL_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "del model  # Free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Training Data\n",
    "\n",
    "Download pretrain data from ModelScope or use sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "DATA_DIR = '/content/mini-llm/data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "PRETRAIN_FILE = f'{DATA_DIR}/pretrain.jsonl'\n",
    "\n",
    "# Try to download from ModelScope, fall back to sample data\n",
    "USE_SAMPLE_DATA = True  # Set to False to use full dataset\n",
    "\n",
    "if not USE_SAMPLE_DATA:\n",
    "    try:\n",
    "        from modelscope import snapshot_download\n",
    "        print(\"Downloading pretrain data from ModelScope...\")\n",
    "        dataset_path = snapshot_download(\n",
    "            'gongjy/minimind_dataset',\n",
    "            cache_dir='/content/cache'\n",
    "        )\n",
    "        PRETRAIN_FILE = os.path.join(dataset_path, 'pretrain_hq.jsonl')\n",
    "        print(f\"Using dataset: {PRETRAIN_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "        USE_SAMPLE_DATA = True\n",
    "\n",
    "if USE_SAMPLE_DATA:\n",
    "    print(\"Creating sample training data...\")\n",
    "    \n",
    "    # Sample Chinese texts for demonstration\n",
    "    sample_texts = [\n",
    "        \"人工智能是计算机科学的一个分支，它致力于创建能够执行通常需要人类智能的任务的系统。\",\n",
    "        \"深度学习是机器学习的一个子领域，它使用多层神经网络来学习数据的复杂表示。\",\n",
    "        \"自然语言处理是人工智能的一个重要领域，研究计算机如何理解和生成人类语言。\",\n",
    "        \"Transformer模型使用自注意力机制来处理序列数据，它是现代大型语言模型的基础架构。\",\n",
    "        \"预训练语言模型通过在大规模文本数据上进行无监督学习，学习语言的通用表示。\",\n",
    "        \"强化学习从人类反馈中学习可以帮助语言模型更好地遵循人类指令和偏好。\",\n",
    "        \"注意力机制允许模型在处理输入时关注最相关的部分，提高了模型的表达能力。\",\n",
    "        \"梯度下降是一种优化算法，通过迭代地调整模型参数来最小化损失函数。\",\n",
    "        \"词嵌入将单词映射到连续的向量空间，使得语义相似的词在空间中距离更近。\",\n",
    "        \"语言模型的困惑度是衡量模型预测能力的指标，困惑度越低表示模型越好。\",\n",
    "    ]\n",
    "    \n",
    "    # Expand dataset for meaningful training\n",
    "    num_samples = 50000  # 50K samples for demo\n",
    "    \n",
    "    with open(PRETRAIN_FILE, 'w', encoding='utf-8') as f:\n",
    "        for i in range(num_samples):\n",
    "            text = sample_texts[i % len(sample_texts)]\n",
    "            # Add some variation\n",
    "            if i % 3 == 0:\n",
    "                text = text + \"这是一个重要的概念。\"\n",
    "            elif i % 3 == 1:\n",
    "                text = \"在现代AI研究中，\" + text\n",
    "            f.write(json.dumps({'text': text}, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"Created {num_samples} training samples at {PRETRAIN_FILE}\")\n",
    "\n",
    "# Count lines\n",
    "with open(PRETRAIN_FILE, 'r') as f:\n",
    "    num_lines = sum(1 for _ in f)\n",
    "print(f\"Total training samples: {num_lines:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "Optimized settings for A100 80GB GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters for A100\n",
    "TRAIN_CONFIG = {\n",
    "    'batch_size': 64,           # A100 can handle larger batches\n",
    "    'accumulation_steps': 4,    # Effective batch = 64 * 4 = 256\n",
    "    'learning_rate': 5e-4,\n",
    "    'epochs': 2,\n",
    "    'max_seq_len': 512,\n",
    "    'dtype': 'bfloat16',        # A100 supports bf16 natively\n",
    "    'grad_clip': 1.0,\n",
    "    'log_interval': 50,\n",
    "    'save_interval': 200,\n",
    "    'num_workers': 4,\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for k, v in TRAIN_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "effective_batch = TRAIN_CONFIG['batch_size'] * TRAIN_CONFIG['accumulation_steps']\n",
    "print(f\"\\nEffective batch size: {effective_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set output directory\n",
    "OUT_DIR = '/content/mini-llm/out/pretrain_0_2b'\n",
    "TB_DIR = '/content/mini-llm/out/pretrain_0_2b/tb'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(TB_DIR, exist_ok=True)\n",
    "\n",
    "# Build training command\n",
    "train_cmd = f\"\"\"\n",
    "python trainer/train_pretrain.py \\\n",
    "    --data_path {PRETRAIN_FILE} \\\n",
    "    --hidden_size {MODEL_CONFIG['hidden_size']} \\\n",
    "    --num_hidden_layers {MODEL_CONFIG['num_hidden_layers']} \\\n",
    "    --max_seq_len {TRAIN_CONFIG['max_seq_len']} \\\n",
    "    --batch_size {TRAIN_CONFIG['batch_size']} \\\n",
    "    --accumulation_steps {TRAIN_CONFIG['accumulation_steps']} \\\n",
    "    --learning_rate {TRAIN_CONFIG['learning_rate']} \\\n",
    "    --epochs {TRAIN_CONFIG['epochs']} \\\n",
    "    --dtype {TRAIN_CONFIG['dtype']} \\\n",
    "    --grad_clip {TRAIN_CONFIG['grad_clip']} \\\n",
    "    --log_interval {TRAIN_CONFIG['log_interval']} \\\n",
    "    --save_interval {TRAIN_CONFIG['save_interval']} \\\n",
    "    --num_workers {TRAIN_CONFIG['num_workers']} \\\n",
    "    --out_dir {OUT_DIR} \\\n",
    "    --tensorboard_dir {TB_DIR} \\\n",
    "    --device cuda:0\n",
    "\"\"\"\n",
    "\n",
    "print(\"Training command:\")\n",
    "print(train_cmd)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "!{train_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitor Training with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/mini-llm/out/pretrain_0_2b/tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from model.model_minillm import MiniLLMConfig, MiniLLMForCausalLM\n",
    "\n",
    "# Load trained model\n",
    "config = MiniLLMConfig(**MODEL_CONFIG)\n",
    "model = MiniLLMForCausalLM(config)\n",
    "\n",
    "checkpoint_path = f\"{OUT_DIR}/pretrain_{MODEL_CONFIG['hidden_size']}.pth\"\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    state_dict = torch.load(checkpoint_path, map_location='cuda')\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"Warning: Checkpoint not found at {checkpoint_path}\")\n",
    "\n",
    "model = model.cuda().eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/content/mini-llm/model/')\n",
    "\n",
    "# Test generation\n",
    "test_prompts = [\n",
    "    \"人工智能\",\n",
    "    \"深度学习是\",\n",
    "    \"在现代科技发展中\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generation Test\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {generated}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save & Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create export package\n",
    "export_dir = '/content/minillm_0_2b_export'\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Copy model checkpoint\n",
    "if os.path.exists(checkpoint_path):\n",
    "    shutil.copy(checkpoint_path, f'{export_dir}/model.pth')\n",
    "    print(f\"Copied model checkpoint\")\n",
    "\n",
    "# Copy tokenizer files\n",
    "tokenizer_files = ['tokenizer.json', 'tokenizer_config.json']\n",
    "for f in tokenizer_files:\n",
    "    src = f'/content/mini-llm/model/{f}'\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, f'{export_dir}/{f}')\n",
    "\n",
    "# Save config\n",
    "import json\n",
    "with open(f'{export_dir}/config.json', 'w') as f:\n",
    "    json.dump(MODEL_CONFIG, f, indent=2)\n",
    "\n",
    "# Create zip\n",
    "!cd /content && zip -r minillm_0_2b.zip minillm_0_2b_export/\n",
    "\n",
    "# List contents\n",
    "print(\"\\nExport contents:\")\n",
    "!ls -lh /content/minillm_0_2b_export/\n",
    "print(f\"\\nZip file size:\")\n",
    "!ls -lh /content/minillm_0_2b.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model (uncomment to download)\n",
    "# from google.colab import files\n",
    "# files.download('/content/minillm_0_2b.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. (Optional) Continue with SFT Training\n",
    "\n",
    "After pretraining, you can fine-tune on instruction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run SFT\n",
    "\"\"\"\n",
    "# Prepare SFT data\n",
    "sft_samples = [\n",
    "    {\"conversations\": [{\"role\": \"user\", \"content\": \"什么是人工智能？\"}, \n",
    "                       {\"role\": \"assistant\", \"content\": \"人工智能是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\"}]},\n",
    "    {\"conversations\": [{\"role\": \"user\", \"content\": \"解释深度学习\"}, \n",
    "                       {\"role\": \"assistant\", \"content\": \"深度学习是机器学习的一个子领域，使用多层神经网络来学习数据的复杂表示。\"}]},\n",
    "]\n",
    "\n",
    "SFT_FILE = '/content/mini-llm/data/sft.jsonl'\n",
    "with open(SFT_FILE, 'w') as f:\n",
    "    for sample in sft_samples * 1000:\n",
    "        f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Run SFT\n",
    "!python trainer/train_full_sft.py \\\n",
    "    --data_path {SFT_FILE} \\\n",
    "    --hidden_size {MODEL_CONFIG['hidden_size']} \\\n",
    "    --num_hidden_layers {MODEL_CONFIG['num_hidden_layers']} \\\n",
    "    --pretrained_path {checkpoint_path} \\\n",
    "    --batch_size 32 \\\n",
    "    --epochs 3 \\\n",
    "    --out_dir /content/mini-llm/out/sft_0_2b\n",
    "\"\"\"\n",
    "print(\"SFT training code is commented out. Uncomment to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook trained a 0.2B parameter MiniLLM model with:\n",
    "- **Architecture**: DeepSeek-V3 style with MLA (Multi-head Latent Attention)\n",
    "- **Parameters**: ~200M\n",
    "- **Training**: 2 epochs on sample data\n",
    "- **Hardware**: A100 80GB GPU\n",
    "\n",
    "For production training, use the full dataset from ModelScope by setting `USE_SAMPLE_DATA = False`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
